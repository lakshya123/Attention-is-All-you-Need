{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5uSM_ozrFOq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Module is the subclass\n",
        "# Generate Random Embeddings\n",
        "class Embedding(nn.Module):\n",
        "\n",
        "  def __init__(self, dict_size, dmodel = 512):\n",
        "    super().__init__()\n",
        "    self.dict_size = dict_size;\n",
        "    self.dmodel = dmodel;\n",
        "    self.embeddings = nn.Embedding(dict_size, dmodel);\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embeddings(x)"
      ],
      "metadata": {
        "id": "hexml3zArLnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Periodic Functions\n",
        "# Bounded Functions\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, max_len, dmodel = 512):\n",
        "    super().__init__();\n",
        "    self.max_len = max_len;\n",
        "    self.dmodel = dmodel;\n",
        "    self.pe = torch.zeros(max_len, dmodel);\n",
        "\n",
        "    pos = torch.arange(0, max_len, dtype = torch.float).unsqueeze(1);\n",
        "    mul_term = torch.pow(10000, -1 * torch.arange(0, dmodel, 2, dtype = torch.float));\n",
        "    self.pe[:, 0::2] = torch.sin(pos * mul_term);\n",
        "    self.pe[:, 1::2] = torch.cos(pos * mul_term);\n",
        "\n",
        "    self.register_buffer('pe', self.pe);\n",
        "\n",
        "  def forward(self, embedding):\n",
        "    return embedding + self.pe;\n"
      ],
      "metadata": {
        "id": "Izks1wP2cqQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dk = dv = dmodel // num_heads\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, dmodel = 512, num_heads = 8):\n",
        "    super().__init__();\n",
        "    self.dmodel = dmodel;\n",
        "    self.num_heads = num_heads;\n",
        "    self.head_dim = dmodel // num_heads;\n",
        "    self.softmax_layer = nn.Softmax(dim = -1);\n",
        "\n",
        "    self.w_key = nn.Linear(dmodel, dmodel);\n",
        "    self.w_query = nn.Linear(dmodel, dmodel);\n",
        "    self.w_value = nn.Linear(dmodel, dmodel);\n",
        "\n",
        "    self.output = nn.Linear(dmodel, dmodel);\n",
        "\n",
        "  # Size of Query / Key / Value : (NB, NH, S/T, HD)\n",
        "  # return Attention Scores : (NB, NH, S/T, HD)\n",
        "  def attention(self, query, key, value, mask = None):\n",
        "\n",
        "    attention_score = torch.matmul(query, key.transpose(-1, -2));\n",
        "    attention_score = attention_score / torch.sqrt(torch.tensor(self.head_dim));\n",
        "\n",
        "    # Replace the masked positions by very small value,\n",
        "    # Softmax for masked positions -> 0\n",
        "    if mask is not None:\n",
        "      attention_score = attention_score.masked_fill(mask == 0, -1e10);\n",
        "\n",
        "    attention_score = self.softmax_layer(attention_score);\n",
        "    attention_score = torch.matmul(attention_score, value);\n",
        "\n",
        "    return attention_score;\n",
        "\n",
        "  # Size of Query / Key / Value : (NB, S/T, ED)\n",
        "  def forward(self, query, key, value, mask = None):\n",
        "\n",
        "    batch_size = query.shape[0];\n",
        "    key = self.w_key(key);\n",
        "    query = self.w_query(query);\n",
        "    value = self.w_value(value);\n",
        "\n",
        "    # Reshape and Transpose for calculating Attention Scores\n",
        "\n",
        "    key = key.reshape(batch_size, -1, self.num_heads, self.head_dim);\n",
        "    query = query.reshape(batch_size, -1, self.num_heads, self.head_dim);\n",
        "    value = value.reshape(batch_size, -1, self.num_heads, self.head_dim);\n",
        "\n",
        "    key = key.transpose(1,2);\n",
        "    query = query.transpose(1,2);\n",
        "    value = value.transpose(1,2);\n",
        "\n",
        "    attention_score = self.attention(query, key, value, mask);\n",
        "    attention_score = attention_score.tranpose(1,2);\n",
        "    attention_score = attention_score.reshape(batch_size, -1, self.dmodel);\n",
        "\n",
        "    return self.output(attention_score);\n",
        "\n"
      ],
      "metadata": {
        "id": "xgxemzTVs8nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# FFN(x) = MAX(xW1 + b1, 0)W2 + b2\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, dmodel = 512, hidden_dim = 2048):\n",
        "    super().__init__();\n",
        "    self.dmodel = dmodel;\n",
        "    self.hidden_dim = hidden_dim;\n",
        "    self.linear1 = nn.Linear(dmodel, hidden_dim);\n",
        "    self.linear2 = nn.Linear(hidden_dim, dmodel);\n",
        "    self.relu = nn.ReLU();\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x);\n",
        "    x = self.relu(x);\n",
        "    x = self.linear2(x);\n",
        "    return x;"
      ],
      "metadata": {
        "id": "Gp1FqFEaxg-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# y(x) = x + Sublayer(x)\n",
        "class Sublayer(nn.Module):\n",
        "  def __init__(self, dmodel = 512):\n",
        "    super().__init__();\n",
        "    self.dmodel = dmodel;\n",
        "    self.norm = nn.LayerNorm(dmodel);\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return self.norm(x + sublayer);"
      ],
      "metadata": {
        "id": "70tjq8cQZG9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, dmodel = 512, num_heads = 8, hidden_layer = 2048):\n",
        "    super().__init__();\n",
        "    self.dmodel = dmodel;\n",
        "\n",
        "    self.multi_head_attention = MultiHeadAttention(dmodel, num_heads);\n",
        "    self.sublayer1 = Sublayer(dmodel);\n",
        "\n",
        "    self.feed_forward_network = FeedForwardNetwork(dmodel, hidden_layer);\n",
        "    self.sublayer2 = Sublayer(dmodel);\n",
        "\n",
        "  def forward(self, vector_in, src_mask = None):\n",
        "\n",
        "    attention_out = self.multi_head_attention(vector_in, vector_in, vector_in, src_mask);\n",
        "    attention_norm = self.sublayer1(vector_in, attention_out);\n",
        "\n",
        "    ffn_out = self.feed_forward_network(attention_norm);\n",
        "    ffn_norm = self.sublayer2(attention_norm, ffn_out);\n",
        "    return ffn_norm;"
      ],
      "metadata": {
        "id": "hSMwzsu8a637"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, num_layers = 6, dmodel = 512, num_heads = 8, hidden_layer = 2048):\n",
        "    super().__init__();\n",
        "    self.num_layers = num_layers;\n",
        "    self.dmodel = dmodel;\n",
        "    self.encoder_layer = EncoderLayer(dmodel, num_heads, hidden_layer);\n",
        "    self.layers = get_clone(self.encoder_layer, num_layers);\n",
        "\n",
        "  def forward(self, vector_in, src_mask = None):\n",
        "    for layer in self.layers:\n",
        "      vector_out = layer(vector_in, src_mask);\n",
        "      vector_in = vector_out;\n",
        "\n",
        "    return vector_out;"
      ],
      "metadata": {
        "id": "1N_utqkSSxoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, dmodel = 512, num_heads = 8, hidden_layer = 2048):\n",
        "    super().__init__();\n",
        "\n",
        "    self.dmodel = dmodel;\n",
        "    self.multi_head_attention1 = MultiHeadAttention(dmodel, num_heads);\n",
        "    self.sublayer1 = Sublayer(dmodel);\n",
        "\n",
        "    self.multi_head_attention2 = MultiHeadAttention(dmodel, num_heads);\n",
        "    self.sublayer2 = Sublayer(dmodel);\n",
        "\n",
        "    self.feed_forward_network = FeedForwardNetwork(dmodel, hidden_layer);\n",
        "    self.sublayer3 = Sublayer(dmodel);\n",
        "\n",
        "  def forward(self, enc_in, dec_in, target_mask):\n",
        "\n",
        "    attention_out1 = self.multi_head_attention1(dec_in, dec_in, dec_in);\n",
        "    attention_norm1 = self.sublayer1(dec_in, attention_out1);\n",
        "\n",
        "    attention_out2 = self.multi_head_attention2(attention_norm1, enc_in, enc_in, target_mask);\n",
        "    attention_norm2 = self.sublayer2(attention_norm1, attention_out2);\n",
        "\n",
        "    ffn_out = self.feed_forward_network(attention_norm2);\n",
        "    ffn_norm = self.sublayer3(ffn_out)\n",
        "\n",
        "    return ffn_norm;\n"
      ],
      "metadata": {
        "id": "wyZs7b79UXkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, num_layers = 6, dmodel = 512, num_heads = 8, hidden_layer = 2048):\n",
        "    super().__init__();\n",
        "    self.dmodel = dmodel;\n",
        "    self.num_layers = num_layers;\n",
        "    self.decoder_layer = DecoderLayer(dmodel, num_heads, hidden_layer);\n",
        "    self.layers = get_clone(self.decoder_layer, num_layers);\n",
        "\n",
        "  def forward(self, enc_in, dec_in, target_mask):\n",
        "    for layer in self.layers:\n",
        "      vector_out = layer(enc_in, dec_in, target_mask);\n",
        "      vector_in = vector_out;\n",
        "    return vector_out;"
      ],
      "metadata": {
        "id": "2X82sMvOWN6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderOutput(nn.Module):\n",
        "  def __init__(self, dmodel):\n",
        "    super().__init__();\n",
        "    self.dmodel = dmodel;\n",
        "    self.linear = nn.Linear(dmodel, dmodel);\n",
        "    self.softmax = nn.Softmax(dim = -1)\n",
        "\n",
        "  def forward(self, target_vec):\n",
        "    dout = self.linear(target_vec);\n",
        "    dout = self.softmax(dout);\n",
        "    return dout;\n"
      ],
      "metadata": {
        "id": "BHQMqy0PWq1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformers(nn.Module):\n",
        "  def __init__(self, src_vocab_size, target_vocab_size, dmodel = 512, num_heads = 8, hidden_layer = 2048, num_layers = 6):\n",
        "    super().__init__();\n",
        "    self.dmodel = dmodel;\n",
        "\n",
        "    self.src_embedding = Embedding(src_vocab_size, dmodel);\n",
        "    self.src_pe = PositionalEncoding(src_vocab_size, dmodel);\n",
        "\n",
        "    self.target_embedding = Embedding(target_vocab_size, dmodel);\n",
        "    self.target_pe = PositionalEncoding(target_vocab_size, dmodel);\n",
        "\n",
        "    self.encoder_block = EncoderBlock(num_layers, dmodel, num_heads, hidden_layer);\n",
        "    self.decoder_block = DecoderBlock(num_layers, dmodel, num_heads, hidden_layer);\n",
        "\n",
        "    self.output = DecoderOutput(dmodel);\n",
        "\n",
        "  def forward(self, src_word_idx, target_word_idx, src_mask = None, target_mask = None):\n",
        "    src_embedding = self.src_embedding(src_word_idx);\n",
        "    src_embedding = self.src_pe(src_embedding);\n",
        "\n",
        "    target_embedding = self.target_embedding(target_word_idx);\n",
        "    target_embedding = self.target_pe(target_embedding);\n",
        "\n",
        "    enc_out = self.encoder_block(src_embedding, src_mask);\n",
        "    dec_out = self.decoder_block(enc_out, target_embedding, target_mask);\n",
        "    output = self.output(dec_out);\n",
        "\n",
        "    return output;"
      ],
      "metadata": {
        "id": "dNNG_zA1fvqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VEFXDZcjnkqu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}